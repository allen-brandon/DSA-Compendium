{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a technique to measure the relationship between a dependent variable and one or more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression ##\n",
    "\n",
    "\n",
    "It does so by drawing a straight line ($mx+b$) through the data, and finding the values for $m$ and $b$ that minimize the distance in $y$ values from the line to each point (aka the loss), at that point's $x$ value.\n",
    "\n",
    "Often each distance from each point's y value is squared:\n",
    "\n",
    "$(y_i - \\hat{y}_i)^2$ ($\\hat{y}_i$ means the predicted value)\n",
    "\n",
    "Then the square root of the sum of all of these squared distances is calculated:\n",
    "\n",
    "$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "This is the loss function, or a measure of how well the line describes the data. There are multiple types of loss functions, and this is arguably the most common one. It's fittingly called Root Mean Squared Error (or RMSE).\n",
    "\n",
    "Here is that same equation, with $\\hat{y}$ expressed in terms of $M$ and $B$ (the actual variables to describe the line):\n",
    "\n",
    "$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (Mx_i + B))^2}$\n",
    "\n",
    "\n",
    "If one can find the minimum for this function, they've found the best fitting line.\n",
    "\n",
    "One could use Gradient Descent to estimate this, but often for basic Linear Regresison, the minimum is directly calculated. This is a matter of calculus; find the partial derivatives for this equation, find the roots (i.e. where the values are 0), and swap the terms around a bit. With MSE (which will always find the same minima as RMSE), we obtain these two equations, to find $M$ and $B$:\n",
    "\n",
    "$M = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$ ($\\bar{x}$ means the average $x$)\n",
    "\n",
    "$B = \\bar{y} - M\\bar{x}$\n",
    "\n",
    "These equations yield the values for $M$ and $B$ that correspond to the line with the minimum error. Then it's just a matter of plugging in the data to the equation. This is $O(n)$, where n is the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression ##\n",
    "\n",
    "When you are finding the relationship between one target variable and multiple independent variables, this is called Multiple Linear Regression. Here, things get a little more complicated. It's essentially the same process, but extruded to three dimensions (or more). So now, instead of finding the closest-fitting line, given a bunch of points on a 2d plane, we're trying to find teh closest-fitting plane, given a bunch of points of a 3d space.\n",
    "\n",
    "Mathematically what that looks like is taking the optimal $mx+b$ line from before, and now trying to find the optimal $m_0x_0 + m_1x_1 + b$. More formally, this can be written as:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$ (Simple Linear Regression)\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n$ (Multiple Linear Regression)\n",
    "\n",
    "Where $\\beta$ refers generally to the coefficient for each term (notably $\\beta_0$ being the y-intercept)\n",
    "\n",
    "This Multiple Linear Regression formula describes a hyperplane; for 2 Dimensions, it's a line describing the relatinoship between$x$ and $y$. for 3 Dimensions, it's a plane \"slicing\" through the 3-dimensional space, describing the relationship between $x_1$, $x_2$ and $y$. For $n$ dimensions, a hyperplane is an $n-1$ dimensional object, that simply predicts $y$, given $x_1, x_2, ... x_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single-Variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-In Library Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Linear Regression estimates a line. To predict the relationship between the weight and volume of an object, this model makes sense, in that there is a roughly linear relationship. For a relationship between the weight and circumference of an object, a nonlinear model would likely be more feasible, since the weight is likely to scale quadratically with the circumference.\n",
    "\n",
    "* MSE, Mean Squared Error, is the primary error function used to find the optimal fit; either through direct analysis, or gradient descent. This finds the exact same minima as RMSE, since the minimum square root of a given function is the same as the minimum of that function when positive (and the MSE can never be negative).\n",
    "\n",
    "* RMSE is typically used to express model accuracy, rather than MSE, because it brings the order of magnitude back to the original terms (rather than squared terms), which yields a more intuitive measure of accuracy.\n",
    "\n",
    "* The primary alternative to MSE/RMSE is taking the absolute difference, rather than the squared difference, between the expected and observed y values:\n",
    "\n",
    "$|y_i - \\hat{y}_i|$\n",
    "\n",
    "The Mean Absolute Difference weighs all cumulative error linearly, as opposed to MSE paying more attention to larger errors. This is a choice that depends on the nature of the data at hand, though weighing larger errors by squaring the residuals is often preferred.\n",
    "\n",
    "* Squaring the residuals serves effectively a dual-purpose; to take the absolute value of the difference (as larger positive/negative $y_i - \\hat{y}_i$ values both mean a larger error), and also to pay more attention to these differences if they are larger. The first is basically always necessary, and the second is often preferred.\n",
    "\n",
    "* Multilinear regression is a distinct term from multivariate linear regression; multilinear regression refers to the aforementioned technique to find the relationship between one dependent variable, and multiple independent variables. Multivariate regression refers generally to a model that estimates multiple dependent variables using potentially multiple independent variables as well. It describes multiple different relationships in a condensed format (such as multiple lines on the same graph).\n",
    "\n",
    "* Q: Why does multilinear regression not give us a line, like simple linear regression does?\n",
    "\n",
    "A: All linear regression produces a hyperplane, which is a way to relate $n-1$ variables to the remaining variable, in $n$ an dimensional space. So for two dimensions (e.g. relating 1 independent variable to 1 dependent variable) this is a line. For three dimensions (e.g. relating 2 independent variables to 1 dependent variable) this produces a plane; for every point on the plane formed by the $x_1, x_2$ axes, there is a corresponding predicted value for the $y$ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
