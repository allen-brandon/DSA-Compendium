{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions ##\n",
    "\n",
    "The function that determines the activation level of each node in a neural network, given its bias, weights and inputs.\n",
    "\n",
    "Can be finite or infinite. Finite functions are preferred because they're bounded, though infinite functions sometimes don't run into as many issues of vanishing gradients, wherein weights and biases become infinitesimally small.\n",
    "\n",
    "Can be differentiable or undifferentiable, \"undifferentiable\" functions must be retrofitted to be differentiable, though, at least when using Gradient Descent. The prominent example of this is RelU (e.g. f(x) = max(0, x)), which isn't differentiable at 0. The derivative for RelU at specific value is chosen arbitrarily, in order to effectively make the function differentiable. Often activation functions are chosen that are not only easy to compute, but also have easy to compute derivatives. For example, the derivative of RelU is a binary comparison with 0, and the derivative of the sigmoid function is the sigmoid function itself; both of these are very fast to compute.\n",
    "\n",
    "Can be linear or nonlinear, though it's almost always nonlinear. A linear activation function is known as the identity function, f(x) = x, and is seldom used. If all the activation functions in a neural network are linear, then the network is linear (e.g. it could be represented with one layer), regardless of how many layers it has.\n",
    "\n",
    "A model of unbounded size with a non-linear activation function has been proven to be a universal function approximator. This is known as the Universal Approximation Theorem.\n",
    "\n",
    "Notably, an activation function *must* be continuous across its domain. In practice it must also be monotonic, though it isn't strictly necessary. If an activation function isn't monotonic, then higher weights can have lower impact, which is the opposite of the standard intended behavior. While this doesn't outright break anything (as with a non-continuous activation function), it does allow the network to not necessarily converge on an optimum, and instead chaotically fluctuate weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relu ####\n",
    "\n",
    "Rectified Linear Activation Function\n",
    "\n",
    "Most popular activation function\n",
    "\n",
    "Combination of two linear functions; f(x)=0 and f(x)=x\n",
    "\n",
    "Very easy to compute function and derivative\n",
    "\n",
    "Not continuously differentiable, which can cause issues with gradient optimization\n",
    "\n",
    "Make sure to use max() or where() over if statements, to remove logical branches\n",
    "\n",
    "The choice to represent relu'(0) as 0 or 1 is often seen as arbitrary, but can be seen as a relevant hyperparameter. David Bertoin et al. (2021) show that relu'(0) = 0 is more stable than relu'(0) = 1. This is largely because of floating point precision errors causing 0-values to be non-negligible, especially in deep models. This relationship is dependent on the structure of the model, as well as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relu\n",
    "\n",
    "import numpy as np\n",
    "#Function\n",
    "relu = lambda x: np.maximum(0,x)\n",
    "\n",
    "#Derivative\n",
    "relu_derivative = lambda x: np.where(x>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid ####\n",
    "\n",
    "Original activation function\n",
    "\n",
    "Also known as logistic activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "#Function\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "\n",
    "#Derivative\n",
    "sigmoid_derivative = lambda x: x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "David Bertoin, Jérôme Bolte, Sébastien Gerchinovitz, Edouard Pauwels. Numerical influence of\n",
    "ReLU’(0) on backpropagation. Advances in Neural Information Processing Systems, Dec 2021, Paris,\n",
    "France. hal-03265059v2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
